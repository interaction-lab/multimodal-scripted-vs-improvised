{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e44306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:30:02.006743: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-06 15:30:02.906196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-05-06 15:30:02.906284: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-05-06 15:30:02.906294: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utt_id</th>\n",
       "      <th>EDA</th>\n",
       "      <th>filename</th>\n",
       "      <th>session_number</th>\n",
       "      <th>speaker</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>utterance</th>\n",
       "      <th>original_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sd</td>\n",
       "      <td>Ses01M_impro07</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>2.6812</td>\n",
       "      <td>7.9800</td>\n",
       "      <td>Check this out.  You know how I've told you I'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "      <td>Ses01M_impro07</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>7.6300</td>\n",
       "      <td>8.5700</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>sd</td>\n",
       "      <td>Ses01M_impro07</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>8.2200</td>\n",
       "      <td>14.7500</td>\n",
       "      <td>Well, this is totally random, I got this full ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>qy</td>\n",
       "      <td>Ses01M_impro07</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>13.9500</td>\n",
       "      <td>21.1200</td>\n",
       "      <td>[LAUGHTER]. For softball? That's unbelievable....</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>qy</td>\n",
       "      <td>Ses01M_impro07</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>15.5400</td>\n",
       "      <td>20.6700</td>\n",
       "      <td>For softball.  They're going to pay me to go t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10034</th>\n",
       "      <td>24</td>\n",
       "      <td>qy</td>\n",
       "      <td>Ses05F_script01_3</td>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>404.1923</td>\n",
       "      <td>406.6600</td>\n",
       "      <td>Do you still feel like that?</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10035</th>\n",
       "      <td>39</td>\n",
       "      <td>sd</td>\n",
       "      <td>Ses05F_script01_3</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>407.9600</td>\n",
       "      <td>410.7823</td>\n",
       "      <td>I I want you now, Annie.</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10036</th>\n",
       "      <td>25</td>\n",
       "      <td>qy</td>\n",
       "      <td>Ses05F_script01_3</td>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>410.4317</td>\n",
       "      <td>427.7079</td>\n",
       "      <td>Because you can't feel like that anymore Chris...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10037</th>\n",
       "      <td>40</td>\n",
       "      <td>sd</td>\n",
       "      <td>Ses05F_script01_3</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>426.7965</td>\n",
       "      <td>431.8242</td>\n",
       "      <td>Oh Annie.  Annie, I am going to make you a for...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10038</th>\n",
       "      <td>26</td>\n",
       "      <td>qw</td>\n",
       "      <td>Ses05F_script01_3</td>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>431.1832</td>\n",
       "      <td>434.1377</td>\n",
       "      <td>What would I do with a fortune?</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10039 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       utt_id EDA           filename  session_number speaker     start  \\\n",
       "0           0  sd     Ses01M_impro07               1       M    2.6812   \n",
       "1           0   b     Ses01M_impro07               1       F    7.6300   \n",
       "2           1  sd     Ses01M_impro07               1       M    8.2200   \n",
       "3           1  qy     Ses01M_impro07               1       F   13.9500   \n",
       "4           2  qy     Ses01M_impro07               1       M   15.5400   \n",
       "...       ...  ..                ...             ...     ...       ...   \n",
       "10034      24  qy  Ses05F_script01_3               5       F  404.1923   \n",
       "10035      39  sd  Ses05F_script01_3               5       M  407.9600   \n",
       "10036      25  qy  Ses05F_script01_3               5       F  410.4317   \n",
       "10037      40  sd  Ses05F_script01_3               5       M  426.7965   \n",
       "10038      26  qw  Ses05F_script01_3               5       F  431.1832   \n",
       "\n",
       "            end                                          utterance  \\\n",
       "0        7.9800  Check this out.  You know how I've told you I'...   \n",
       "1        8.5700                                              Yeah.   \n",
       "2       14.7500  Well, this is totally random, I got this full ...   \n",
       "3       21.1200  [LAUGHTER]. For softball? That's unbelievable....   \n",
       "4       20.6700  For softball.  They're going to pay me to go t...   \n",
       "...         ...                                                ...   \n",
       "10034  406.6600                       Do you still feel like that?   \n",
       "10035  410.7823                           I I want you now, Annie.   \n",
       "10036  427.7079  Because you can't feel like that anymore Chris...   \n",
       "10037  431.8242  Oh Annie.  Annie, I am going to make you a for...   \n",
       "10038  434.1377                    What would I do with a fortune?   \n",
       "\n",
       "       original_order  \n",
       "0                   0  \n",
       "1                   1  \n",
       "2                   2  \n",
       "3                   3  \n",
       "4                   4  \n",
       "...               ...  \n",
       "10034              63  \n",
       "10035              64  \n",
       "10036              65  \n",
       "10037              66  \n",
       "10038              67  \n",
       "\n",
       "[10039 rows x 9 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # cool M2 chip GPU acceleration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # for NVIDIA GPUs\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# 1. Access the corresponding .txt, .wav, and .avi files for each EDA label\n",
    "# Extract the conversation filename and speaker informationfrom the dataset\n",
    "eda_df = pd.read_csv(\"eda_iemocap_no_utts_dataset.csv\")\n",
    "eda_df = eda_df[[\"speaker\", \"utt_id\", \"EDA\"]]\n",
    "filename_ids = []\n",
    "speaker_M_F = []\n",
    "session_numbers = []\n",
    "for i, row in eda_df.iterrows():\n",
    "    match = re.search(r\"b'(Ses(\\d+)[MF]_.+\\d+.*)_([MF])\", row[\"speaker\"])\n",
    "    filename_ids.append(match.group(1))\n",
    "    session_numbers.append(int(match.group(2))) \n",
    "    speaker_M_F.append(match.group(3))\n",
    "eda_df = eda_df.drop(columns=[\"speaker\"])\n",
    "eda_df[\"filename\"] = filename_ids\n",
    "eda_df[\"filename\"] = eda_df[\"filename\"].astype(str)\n",
    "eda_df[\"session_number\"] = session_numbers\n",
    "eda_df[\"session_number\"] = eda_df[\"session_number\"].astype(int)\n",
    "eda_df[\"speaker\"] = speaker_M_F\n",
    "eda_df[\"speaker\"] = eda_df[\"speaker\"].astype(str)\n",
    "eda_df[\"utt_id\"] = eda_df[\"utt_id\"].astype(int)\n",
    "# Access transcipt files based on filename\n",
    "utt_df = []\n",
    "root_dir = \"IEMOCAP_full_release/\"\n",
    "for i in range(1, 6):\n",
    "    directory = os.path.join(root_dir, f\"Session{i}/dialog/transcriptions/\")\n",
    "    for entry in os.scandir(directory):  \n",
    "        if entry.is_file() and entry.path.endswith(\".txt\"):  # check if it's a file\n",
    "            try:\n",
    "                with open(entry.path, \"r\") as file:\n",
    "                    filename = entry.path.split(\"/\")[-1][:-4]\n",
    "                    lines = file.readlines()\n",
    "                    for order, line in enumerate(lines):\n",
    "                        speaker_info, utterance = line.split(\":\")[0], line.split(\":\")[1]\n",
    "                        pattern = r\"(F|M)(\\d+)\\s\\[(\\d+\\.\\d+)-(\\d+\\.\\d+)\\]\"\n",
    "                        match = re.search(pattern, speaker_info)\n",
    "                        if match is None:\n",
    "                            continue\n",
    "                        speaker_f_m = match.group(1)\n",
    "                        utt_id = match.group(2)\n",
    "                        start = match.group(3)\n",
    "                        end = match.group(4)\n",
    "                        utt_df.append({\"utt_id\": int(utt_id), \"filename\": str(filename), \"start\": float(start), \"end\": float(end), \"speaker\": str(speaker_f_m.strip()), \"utterance\": utterance.strip(), \"session_number\": int(i), \"original_order\": order})\n",
    "            except:\n",
    "                #print(entry.path) # these are meta files with ._ prepended to text file name\n",
    "                continue\n",
    "utt_df = pd.DataFrame(utt_df)\n",
    "# Combine the EDA and utterances together\n",
    "final_df = pd.merge(eda_df, utt_df, on=[\"utt_id\", \"session_number\", \"filename\", \"speaker\"])\n",
    "final_df\n",
    "# TODO: Double check if there is really a perfect merge match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c80592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/gpuspock/anaconda3/envs/llm4cbt-empathy/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92737179e787457ab2f5c1278b112e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# 2. Create a dataset following agreement estimation project for each modality or each utterance\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "labels = list(set(Counter(final_df[\"EDA\"]).keys())) # there are 34 labels\n",
    "labels_to_num_mapping = {}#'CONCESSION': 0, 'FACTS': 1, 'INTEREST': 2, 'POSITIVE EXPECTATIONS': 3, 'POWER': 4, 'PROCEDURAL': 5, 'PROPOSAL': 6, 'RESIDUAL': 7, 'RIGHTS': 8}\n",
    "for i, label in enumerate(labels):\n",
    "    labels_to_num_mapping[label] = i\n",
    "\n",
    "class DialogActDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(labels_to_num_mapping[self.labels[idx]])\n",
    "        return item     \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "df_train, df_test = train_test_split(final_df, train_size=0.8)\n",
    "df_test, df_val = train_test_split(df_test, train_size=0.5)\n",
    "\n",
    "train_texts, train_labels = [utt.lower() for utt in list(df_train[\"utterance\"])], list(df_train[\"EDA\"])\n",
    "val_texts, val_labels = [utt.lower() for utt in list(df_val[\"utterance\"])], list(df_val[\"EDA\"])\n",
    "test_texts, test_labels = [utt.lower() for utt in list(df_test[\"utterance\"])], list(df_test[\"EDA\"])\n",
    "\n",
    "model_card = \"FacebookAI/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = DialogActDataset(train_encodings, train_labels)\n",
    "val_dataset = DialogActDataset(val_encodings, val_labels)\n",
    "test_dataset = DialogActDataset(test_encodings, test_labels)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_card, num_labels=len(labels))\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/roberta-large/\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01#,\n",
    "    # eval_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# # 3. Run some training on 80% of the scripted data on the lab machine\n",
    "# # 4. Determine the training/test splits (think of emotion distribution, scenarios, speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de095d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df580be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4cbt-empathy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
